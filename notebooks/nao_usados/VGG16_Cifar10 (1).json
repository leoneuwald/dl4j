{"paragraphs":[{"text":"%spark.dep\nz.reset()\nz.load(\"org.nd4j:nd4j-native-platform:0.9.1\")\nz.load(\"org.deeplearning4j:deeplearning4j-core:0.9.1\")\nz.load(\"org.datavec:datavec-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:dl4j-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:deeplearning4j-zoo:0.9.1\")","user":"anonymous","dateUpdated":"2019-09-01T05:14:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@db18668\n"}]},"apps":[],"jobName":"paragraph_1567312570993_1494442209","id":"20190813-010616_2021083357","dateCreated":"2019-09-01T04:36:10+0000","dateStarted":"2019-09-01T05:14:16+0000","dateFinished":"2019-09-01T05:14:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:15055"},{"text":"import scala.collection.JavaConversions._\n\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.apache.spark.SparkConf\nimport scala.collection.mutable.ListBuffer\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.graph.SparkComputationGraph\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.dataset.DataSet\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n\n","user":"anonymous","dateUpdated":"2019-09-01T05:43:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scala.collection.JavaConversions._\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.apache.spark.SparkConf\nimport scala.collection.mutable.ListBuffer\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.graph.SparkComputationGraph\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.dataset.DataSet\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n"}]},"apps":[],"jobName":"paragraph_1567312570998_1494057460","id":"20190813-010626_1430679682","dateCreated":"2019-09-01T04:36:10+0000","dateStarted":"2019-09-01T05:43:20+0000","dateFinished":"2019-09-01T05:43:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15056"},{"text":"import org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator\n\nval emnistSet = EmnistDataSetIterator.Set.LETTERS\n\nval numberOfLabels = EmnistDataSetIterator.numLabels(emnistSet)\n\nval batchSize = 500 // how many examples to simultaneously train in the network\n\nval emnistTrain = new EmnistDataSetIterator(emnistSet, batchSize, true)\nval emnistTest = new EmnistDataSetIterator(emnistSet, batchSize, false)\n\n\nimport scala.collection.mutable.ListBuffer\n\nvar trainDataList = new ListBuffer[DataSet]()\nwhile (emnistTrain.hasNext()) {\n  trainDataList += emnistTrain.next()\n}\n\nval testDataList = new ListBuffer[DataSet]()\nwhile (emnistTest.hasNext()) {\n  testDataList += emnistTest.next()\n}\nval trainData = sc.parallelize(trainDataList)\nval testData = sc.parallelize(testDataList)","user":"anonymous","dateUpdated":"2019-09-01T05:43:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator\nemnistSet: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator.Set = LETTERS\nnumberOfLabels: Int = 26\nbatchSize: Int = 500\nemnistTrain: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator@70357f0b\nemnistTest: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator@23407d5a\nimport scala.collection.mutable.ListBuffer\ntrainDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntestDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntrainData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[0] at parallelize at <console>:91\ntestData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[1] at parallelize at <console>:91\n"}]},"apps":[],"jobName":"paragraph_1567312570999_1493672711","id":"20190828-044726_487790819","dateCreated":"2019-09-01T04:36:10+0000","dateStarted":"2019-09-01T05:43:26+0000","dateFinished":"2019-09-01T05:43:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15057"},{"text":"import org.deeplearning4j.zoo.model.SimpleCNN\nval seed = 1234 // integer for reproducability of a random number generator\n\nval zooModel = new SimpleCNN(numberOfLabels, seed, 3)","user":"anonymous","dateUpdated":"2019-09-01T05:46:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.zoo.model.SimpleCNN\nseed: Int = 1234\nzooModel: org.deeplearning4j.zoo.model.SimpleCNN = org.deeplearning4j.zoo.model.SimpleCNN@20dcf21e\n"}]},"apps":[],"jobName":"paragraph_1567312571000_1491748967","id":"20190813-014219_2027485131","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T05:46:56+0000","dateFinished":"2019-09-01T05:46:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15058"},{"text":"import collection.JavaConverters._\nzooModel.setInputShape(Array(Array(1, 28, 28)))","user":"anonymous","dateUpdated":"2019-09-01T05:52:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import collection.JavaConverters._\n"}]},"apps":[],"jobName":"paragraph_1567312571001_1491364218","id":"20190828-024936_683229252","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T05:51:29+0000","dateFinished":"2019-09-01T05:51:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15059"},{"text":"val tm = new ParameterAveragingTrainingMaster.Builder(batchSize)    //Each DataSet object: contains (by default) 32 examples\n  .workerPrefetchNumBatches(0)\n  .saveUpdater(true)\n  .averagingFrequency(5)                            //Do 5 minibatch fit operations per worker, then average and redistribute parameters\n  .batchSizePerWorker(batchSize)     //Number of examples that each worker uses per fit operation\n  .build()\n  \nval sparkNet = new SparkDl4jMultiLayer(sc, zooModel.conf(), tm)","user":"anonymous","dateUpdated":"2019-09-01T05:51:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"tm: org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster = ParameterAveragingTrainingMaster(saveUpdater=true, numWorkers=null, rddDataSetNumExamples=500, averagingFrequency=5, aggregationDepth=2, prefetchNumBatches=0, iterationCount=0, trainingHookList=null)\norg.deeplearning4j.exception.DL4JInvalidConfigException: Invalid configuration for layer (idx=0, name=image_array, type=ConvolutionLayer) for height dimension:  Invalid input configuration for kernel height. Require 0 < kH <= inHeight + 2*padH; got (kH=7, inHeight=1, padH=0)\nInput type = InputTypeConvolutional(h=1,w=28,d=28), kernel = [7, 7], strides = [1, 1], padding = [0, 0], layer size (output depth) = 16, convolution mode = Same\n  at org.deeplearning4j.nn.conf.layers.InputTypeUtil.getOutputTypeCnnLayers(InputTypeUtil.java:53)\n  at org.deeplearning4j.nn.conf.layers.ConvolutionLayer.getOutputType(ConvolutionLayer.java:147)\n  at org.deeplearning4j.nn.conf.MultiLayerConfiguration$Builder.build(MultiLayerConfiguration.java:543)\n  at org.deeplearning4j.nn.conf.NeuralNetConfiguration$ListBuilder.build(NeuralNetConfiguration.java:285)\n  at org.deeplearning4j.zoo.model.SimpleCNN.conf(SimpleCNN.java:131)\n  ... 84 elided\n"}]},"apps":[],"jobName":"paragraph_1567312571001_1491364218","id":"20190819-011525_331240530","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T05:51:32+0000","dateFinished":"2019-09-01T05:51:33+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:15060"},{"text":"val numEpochs = 2\nfor (i <- 0 to numEpochs) {\n  println(i)\n  val trained = sparkNet.fit(trainData)\n}","user":"anonymous","dateUpdated":"2019-09-01T05:47:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"numEpochs: Int = 2\n0\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 10, emnist-cluster-w-5.us-central1-b.c.hidden-analyzer-249419.internal): org.deeplearning4j.exception.DL4JInvalidInputException: Got rank 2 array as input to ConvolutionLayer (layer name = image_array, layer index = 0) with shape [500, 784]. Expected rank 4 array with shape [minibatchSize, layerInputDepth, inputHeight, inputWidth]. (Wrong input type (see InputType.convolutionalFlat()) or wrong data type?) (layer name: image_array, layer index: 0)\n\tat org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:279)\n\tat org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:248)\n\tat org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)\n\tat org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:309)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:789)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2224)\n\tat org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:174)\n\tat org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)\n\tat org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1780)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1729)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1832)\n\tat org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:179)\n\tat org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:41)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:125)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:42)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:71)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:38)\n\tat org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)\n  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)\n  at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438)\n  at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.processResults(ParameterAveragingTrainingMaster.java:801)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.doIterationPaths(ParameterAveragingTrainingMaster.java:712)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.executeTrainingPathsHelper(ParameterAveragingTrainingMaster.java:432)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.executeTraining(ParameterAveragingTrainingMaster.java:317)\n  at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:218)\n  at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:205)\n  at $$$$4f1dd1244917239c21cdfe76974c148$$$$anonfun$1.apply$mcVI$sp(<console>:139)\n  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n  ... 80 elided\nCaused by: org.deeplearning4j.exception.DL4JInvalidInputException: Got rank 2 array as input to ConvolutionLayer (layer name = image_array, layer index = 0) with shape [500, 784]. Expected rank 4 array with shape [minibatchSize, layerInputDepth, inputHeight, inputWidth]. (Wrong input type (see InputType.convolutionalFlat()) or wrong data type?) (layer name: image_array, layer index: 0)\n  at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:279)\n  at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:248)\n  at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)\n  at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:309)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:789)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2224)\n  at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:174)\n  at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)\n  at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1780)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1729)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1832)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:179)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:41)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:125)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:42)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:71)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:38)\n  at org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:86)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1567312571002_1492518464","id":"20190813-015330_1330142053","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T05:47:31+0000","dateFinished":"2019-09-01T05:48:05+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:15061"},{"text":"val resultado = sparkNet.doEvaluation(testData, 64, new Evaluation(10))(0)","user":"anonymous","dateUpdated":"2019-09-01T04:37:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"resultado: org.deeplearning4j.eval.Evaluation =\n\nExamples labeled as 0 classified by model as 0: 1000 times\nExamples labeled as 1 classified by model as 0: 1000 times\nExamples labeled as 2 classified by model as 0: 1000 times\nExamples labeled as 3 classified by model as 0: 1000 times\nExamples labeled as 4 classified by model as 0: 1000 times\nExamples labeled as 5 classified by model as 0: 1000 times\nExamples labeled as 6 classified by model as 0: 1000 times\nExamples labeled as 7 classified by model as 0: 1000 times\nExamples labeled as 8 classified by model as 0: 1000 times\nExamples labeled as 9 classified by model as 0: 1000 times\n\nWarning: 9 classes were never predicted by the model and were excluded from average precision\nClasses excluded from average precision: [1, 2, 3, 4, 5, 6, 7, ..."}]},"apps":[],"jobName":"paragraph_1567312571002_1492518464","id":"20190813-021401_2110373941","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T04:38:38+0000","dateFinished":"2019-09-01T05:10:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15062"},{"text":"println(resultado)","user":"anonymous","dateUpdated":"2019-09-01T04:37:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nExamples labeled as 0 classified by model as 0: 1000 times\nExamples labeled as 1 classified by model as 0: 1000 times\nExamples labeled as 2 classified by model as 0: 1000 times\nExamples labeled as 3 classified by model as 0: 1000 times\nExamples labeled as 4 classified by model as 0: 1000 times\nExamples labeled as 5 classified by model as 0: 1000 times\nExamples labeled as 6 classified by model as 0: 1000 times\nExamples labeled as 7 classified by model as 0: 1000 times\nExamples labeled as 8 classified by model as 0: 1000 times\nExamples labeled as 9 classified by model as 0: 1000 times\n\nWarning: 9 classes were never predicted by the model and were excluded from average precision\nClasses excluded from average precision: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n==========================Scores========================================\n # of classes:    10\n Accuracy:        0.1000\n Precision:       0.1000\t(9 classes excluded from average)\n Recall:          0.1000\n F1 Score:        0.1818\t(9 classes excluded from average)\nPrecision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)\n========================================================================\n"}]},"apps":[],"jobName":"paragraph_1567312571002_1492518464","id":"20190828-032002_1964617380","dateCreated":"2019-09-01T04:36:11+0000","dateStarted":"2019-09-01T05:09:29+0000","dateFinished":"2019-09-01T05:10:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:15063"},{"user":"anonymous","dateUpdated":"2019-09-01T04:37:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1567312571003_1492133715","id":"20190828-032119_912714771","dateCreated":"2019-09-01T04:36:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:15064"}],"name":"VGG16_Cifar10","id":"2EKQCCWAP","angularObjects":{"2EPDRVGTS:shared_process":[],"2EKJD58AT:shared_process":[],"2EMVQHY36:shared_process":[],"2ENHECCTJ:shared_process":[],"2EN47CQDU:shared_process":[],"2EMRGU8RN:shared_process":[],"2EM2DMURB:shared_process":[],"2EMNMC5PP:shared_process":[],"2ENM73YU2:shared_process":[],"2EN4HBAC6:shared_process":[],"2EM8T6CFH:shared_process":[],"2EMYHCVW3:shared_process":[],"2EMCDKJYF:shared_process":[],"2ENGM55BK:shared_process":[],"2EJSR7ZFP:shared_process":[],"2ENT87J69:shared_process":[],"2EK6E7AAQ:shared_process":[],"2EKYUJKGS:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}