{"paragraphs":[{"text":"%spark.dep\nz.reset()\nz.load(\"org.nd4j:nd4j-native-platform:0.9.1\")\nz.load(\"org.deeplearning4j:deeplearning4j-core:0.9.1\")\nz.load(\"org.datavec:datavec-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:dl4j-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:deeplearning4j-zoo:0.9.1\")","user":"anonymous","dateUpdated":"2019-08-30T05:49:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@14d63c65\n"}]},"apps":[],"jobName":"paragraph_1567144145565_125111505","id":"20190813-010616_2021083357","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:49:26+0000","dateFinished":"2019-08-30T05:49:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2817"},{"text":"import scala.collection.JavaConversions._\n\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.deeplearning4j.nn.api._\nimport org.deeplearning4j.nn.multilayer._\nimport org.deeplearning4j.nn.graph._\nimport org.deeplearning4j.nn.conf._\nimport org.deeplearning4j.nn.conf.inputs._\nimport org.deeplearning4j.nn.conf.layers._\nimport org.deeplearning4j.nn.weights._\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.deeplearning4j.optimize.listeners._\nimport org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.zoo.model.LeNet\n\nimport org.nd4j.linalg.learning.config._ // for different updaters like Adam, Nesterovs, etc.\nimport org.nd4j.linalg.activations.Activation // defines different activation functions like RELU, SOFTMAX, etc.\nimport org.nd4j.linalg.lossfunctions.LossFunctions // mean squared error, multiclass cross entropy, etc.\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n\n","user":"anonymous","dateUpdated":"2019-08-30T05:49:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scala.collection.JavaConversions._\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.deeplearning4j.nn.api._\nimport org.deeplearning4j.nn.multilayer._\nimport org.deeplearning4j.nn.graph._\nimport org.deeplearning4j.nn.conf._\nimport org.deeplearning4j.nn.conf.inputs._\nimport org.deeplearning4j.nn.conf.layers._\nimport org.deeplearning4j.nn.weights._\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.deeplearning4j.optimize.listeners._\nimport org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.zoo.model.LeNet\nimport org.nd4j.linalg.learning.config._\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.apache.spark.SparkConf\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n"}]},"apps":[],"jobName":"paragraph_1567144145571_112030042","id":"20190813-010626_1430679682","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:49:27+0000","dateFinished":"2019-08-30T05:50:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2818"},{"text":"import org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator\n\nval emnistSet = EmnistDataSetIterator.Set.COMPLETE\n\nval numberOfLabels = EmnistDataSetIterator.numLabels(emnistSet)\n\nval batchSize = 16 // how many examples to simultaneously train in the network\n\nval emnistTrain = new EmnistDataSetIterator(emnistSet, batchSize, true)\nval emnistTest = new EmnistDataSetIterator(emnistSet, batchSize, false)\n\n\nimport scala.collection.mutable.ListBuffer\n\nvar trainDataList = new ListBuffer[DataSet]()\nwhile (emnistTrain.hasNext()) {\n  trainDataList += emnistTrain.next()\n}\n\nval testDataList = new ListBuffer[DataSet]()\nwhile (emnistTest.hasNext()) {\n  testDataList += emnistTest.next()\n}\nval trainData = sc.parallelize(trainDataList)\nval testData = sc.parallelize(testDataList)","user":"anonymous","dateUpdated":"2019-08-30T05:49:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator\nemnistSet: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator.Set = COMPLETE\nnumberOfLabels: Int = 62\nbatchSize: Int = 16\nemnistTrain: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator@67660791\nemnistTest: org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator@3ef18021\nimport scala.collection.mutable.ListBuffer\ntrainDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntestDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntrainData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[0] at parallelize at <console>:96\ntestData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[1] at parallelize at <console>:96\n"}]},"apps":[],"jobName":"paragraph_1567144145571_112030042","id":"20190828-044805_1310907596","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:49:47+0000","dateFinished":"2019-08-30T05:50:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2819"},{"text":"val seed = 123 // integer for reproducability of a random number generator\n\nval zooModel = new LeNet(numberOfLabels, seed, 3)","user":"anonymous","dateUpdated":"2019-08-30T05:49:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"seed: Int = 123\nzooModel: org.deeplearning4j.zoo.model.LeNet = org.deeplearning4j.zoo.model.LeNet@70bd8a07\n"}]},"apps":[],"jobName":"paragraph_1567144145572_110106298","id":"20190813-014219_2027485131","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:50:10+0000","dateFinished":"2019-08-30T05:50:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2820"},{"text":"import collection.JavaConverters._\nzooModel.setInputShape(Array(Array(1, 28, 28)))","user":"anonymous","dateUpdated":"2019-08-30T06:18:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import collection.JavaConverters._\n"}]},"apps":[],"jobName":"paragraph_1567144145572_110106298","id":"20190828-024936_683229252","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T06:18:44+0000","dateFinished":"2019-08-30T06:18:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2821"},{"text":"val tm = new ParameterAveragingTrainingMaster.Builder(batchSize)    //Each DataSet object: contains (by default) 32 examples\n  .workerPrefetchNumBatches(0)\n  .saveUpdater(true)\n  .averagingFrequency(5)                            //Do 5 minibatch fit operations per worker, then average and redistribute parameters\n  .batchSizePerWorker(batchSize)     //Number of examples that each worker uses per fit operation\n  .build()\n  \nval sparkNet = new SparkDl4jMultiLayer(sc, zooModel.conf(), tm)","user":"anonymous","dateUpdated":"2019-08-30T06:18:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"tm: org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster = ParameterAveragingTrainingMaster(saveUpdater=true, numWorkers=null, rddDataSetNumExamples=16, averagingFrequency=5, aggregationDepth=2, prefetchNumBatches=0, iterationCount=0, trainingHookList=null)\nsparkNet: org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer = org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer@3157e763\n"}]},"apps":[],"jobName":"paragraph_1567144145573_109721549","id":"20190819-011525_331240530","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T06:18:47+0000","dateFinished":"2019-08-30T06:18:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2822"},{"text":"val numEpochs = 6\nfor (i <- 0 to numEpochs) {\n  val trained = sparkNet.fit(trainData)\n}","user":"anonymous","dateUpdated":"2019-08-30T06:18:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"msg":[{"data":"","type":"TEXT"}]},"apps":[],"jobName":"paragraph_1567144145573_109721549","id":"20190813-015330_1330142053","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T06:18:51+0000","dateFinished":"2019-08-30T06:07:39+0000","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:2823","errorMessage":""},{"text":"val resultado = sparkNet.doEvaluation(testData, 64, new Evaluation(10))(0)","user":"anonymous","dateUpdated":"2019-08-30T05:49:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 5.0 failed 4 times, most recent failure: Lost task 4.3 in stage 5.0 (TID 109, emnist-cluster-w-1.us-central1-b.c.hidden-analyzer-249419.internal): java.lang.IllegalArgumentException: Invalid input: expect output columns must be equal to rows 32 x columns 32 x channels 3 but was instead [64, 784]\n\tat org.deeplearning4j.nn.conf.preprocessor.FeedForwardToCnnPreProcessor.preProcess(FeedForwardToCnnPreProcessor.java:94)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:788)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:870)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:861)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.silentOutput(MultiLayerNetwork.java:1906)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.doEvaluation(MultiLayerNetwork.java:2894)\n\tat org.deeplearning4j.spark.impl.multilayer.evaluation.IEvaluateFlatMapFunctionAdapter.call(IEvaluateFlatMapFunction.java:117)\n\tat org.deeplearning4j.spark.impl.multilayer.evaluation.IEvaluateFlatMapFunctionAdapter.call(IEvaluateFlatMapFunction.java:62)\n\tat org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)\n  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)\n  at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438)\n  at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45)\n  at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:448)\n  at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45)\n  at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.doEvaluation(SparkDl4jMultiLayer.java:598)\n  ... 72 elided\nCaused by: java.lang.IllegalArgumentException: Invalid input: expect output columns must be equal to rows 32 x columns 32 x channels 3 but was instead [64, 784]\n  at org.deeplearning4j.nn.conf.preprocessor.FeedForwardToCnnPreProcessor.preProcess(FeedForwardToCnnPreProcessor.java:94)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:788)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:870)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:861)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.silentOutput(MultiLayerNetwork.java:1906)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.doEvaluation(MultiLayerNetwork.java:2894)\n  at org.deeplearning4j.spark.impl.multilayer.evaluation.IEvaluateFlatMapFunctionAdapter.call(IEvaluateFlatMapFunction.java:117)\n  at org.deeplearning4j.spark.impl.multilayer.evaluation.IEvaluateFlatMapFunctionAdapter.call(IEvaluateFlatMapFunction.java:62)\n  at org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n  at org.apache.spark.scheduler.Task.run(Task.scala:86)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1567144145574_110875796","id":"20190813-021401_2110373941","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:50:55+0000","dateFinished":"2019-08-30T05:54:03+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2824"},{"text":"println(resultado)","user":"anonymous","dateUpdated":"2019-08-30T05:49:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:96: error: not found: value resultado\n       println(resultado)\n               ^\n"}]},"apps":[],"jobName":"paragraph_1567144145574_110875796","id":"20190828-032002_1964617380","dateCreated":"2019-08-30T05:49:05+0000","dateStarted":"2019-08-30T05:53:50+0000","dateFinished":"2019-08-30T05:54:04+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2825"},{"user":"anonymous","dateUpdated":"2019-08-30T05:49:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1567144145575_110491047","id":"20190828-032119_912714771","dateCreated":"2019-08-30T05:49:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2826"}],"name":"LeNet - EmnistComplete","id":"2EKCEDT4E","angularObjects":{"2ENF5Y7VY:shared_process":[],"2EME2UEQB:shared_process":[],"2EMASCTQK:shared_process":[],"2EKJ4J81Y:shared_process":[],"2EP66WQG9:shared_process":[],"2EKXK4BAU:shared_process":[],"2EMYZ4BA8:shared_process":[],"2EMEG2WDS:shared_process":[],"2EP4UZ8X7:shared_process":[],"2EMEGC8W1:shared_process":[],"2ENGYKN22:shared_process":[],"2ENT8HP6R:shared_process":[],"2ENGQ3D95:shared_process":[],"2EKZ5QK42:shared_process":[],"2EK3NV61P:shared_process":[],"2EJWWDP9F:shared_process":[],"2EN5E2TVK:shared_process":[],"2EP7TKFW6:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}