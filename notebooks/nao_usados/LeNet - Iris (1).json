{"paragraphs":[{"text":"%spark.dep\nz.reset()\nz.load(\"org.nd4j:nd4j-native-platform:0.9.1\")\nz.load(\"org.deeplearning4j:deeplearning4j-core:0.9.1\")\nz.load(\"org.datavec:datavec-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:dl4j-spark_2.11:0.9.1_spark_2\")\nz.load(\"org.deeplearning4j:deeplearning4j-zoo:0.9.1\")","dateUpdated":"2019-08-29T06:51:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@1568e180\n"}]},"apps":[],"jobName":"paragraph_1567057651023_1129879177","id":"20190813-010616_2021083357","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:403","user":"anonymous","dateFinished":"2019-08-29T06:51:24+0000","dateStarted":"2019-08-29T06:51:14+0000"},{"text":"import scala.collection.JavaConversions._\n\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.deeplearning4j.nn.api._\nimport org.deeplearning4j.nn.multilayer._\nimport org.deeplearning4j.nn.graph._\nimport org.deeplearning4j.nn.conf._\nimport org.deeplearning4j.nn.conf.inputs._\nimport org.deeplearning4j.nn.conf.layers._\nimport org.deeplearning4j.nn.weights._\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.deeplearning4j.optimize.listeners._\nimport org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.zoo.model.LeNet\n\nimport org.nd4j.linalg.learning.config._ // for different updaters like Adam, Nesterovs, etc.\nimport org.nd4j.linalg.activations.Activation // defines different activation functions like RELU, SOFTMAX, etc.\nimport org.nd4j.linalg.lossfunctions.LossFunctions // mean squared error, multiclass cross entropy, etc.\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n\n","dateUpdated":"2019-08-29T06:51:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scala.collection.JavaConversions._\nimport org.deeplearning4j.datasets.iterator._\nimport org.deeplearning4j.datasets.iterator.impl._\nimport org.deeplearning4j.nn.api._\nimport org.deeplearning4j.nn.multilayer._\nimport org.deeplearning4j.nn.graph._\nimport org.deeplearning4j.nn.conf._\nimport org.deeplearning4j.nn.conf.inputs._\nimport org.deeplearning4j.nn.conf.layers._\nimport org.deeplearning4j.nn.weights._\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.deeplearning4j.optimize.listeners._\nimport org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.zoo.model.LeNet\nimport org.nd4j.linalg.learning.config._\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.apache.spark.SparkConf\nimport org.apache.spark.api.java.JavaRDD\nimport org.apache.spark.api.java.JavaSparkContext\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.datavec.image.loader.CifarLoader\nimport org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator\n"}]},"apps":[],"jobName":"paragraph_1567057651024_1140267397","id":"20190813-010626_1430679682","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:404","user":"anonymous","dateFinished":"2019-08-29T06:51:53+0000","dateStarted":"2019-08-29T06:51:15+0000"},{"text":"val numSamples = 150\nval batchSize = 150","dateUpdated":"2019-08-29T06:51:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"numSamples: Int = 150\nbatchSize: Int = 150\n"}]},"apps":[],"jobName":"paragraph_1567057651024_1140267397","id":"20190828-044726_487790819","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:405","user":"anonymous","dateFinished":"2019-08-29T06:51:53+0000","dateStarted":"2019-08-29T06:51:25+0000"},{"text":"val trainingDataSet = new IrisDataSetIterator(batchSize, numSamples)\nval testDataSet = new IrisDataSetIterator(batchSize, numSamples)\nimport scala.collection.mutable.ListBuffer\n\nvar trainDataList = new ListBuffer[DataSet]()\nwhile (trainingDataSet.hasNext()) {\n  trainDataList += trainingDataSet.next()\n}\n\nval testDataList = new ListBuffer[DataSet]()\nwhile (testDataSet.hasNext()) {\n  testDataList += testDataSet.next()\n}\nval trainData = sc.parallelize(trainDataList)\nval testData = sc.parallelize(testDataList)","dateUpdated":"2019-08-29T06:55:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"trainingDataSet: org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator = org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator@72df63f6\ntestDataSet: org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator = org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator@2274fa87\nimport scala.collection.mutable.ListBuffer\ntrainDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntestDataList: scala.collection.mutable.ListBuffer[org.nd4j.linalg.dataset.DataSet] = ListBuffer()\ntrainData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[0] at parallelize at <console>:101\ntestData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[1] at parallelize at <console>:101\n"}]},"apps":[],"jobName":"paragraph_1567057651024_1140267397","id":"20190828-044805_1310907596","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:407","user":"anonymous","dateFinished":"2019-08-29T06:55:25+0000","dateStarted":"2019-08-29T06:55:21+0000"},{"text":"//val classesNum = MnistDataSetIterator.numLabels(emnistSet) // total output classes\nval seed = 123 // integer for reproducability of a random number generator\n\nval numberOfLabels = 3\n\nval zooModel = new LeNet(numberOfLabels, seed, 3)","dateUpdated":"2019-08-29T06:53:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"seed: Int = 123\nnumberOfLabels: Int = 3\nzooModel: org.deeplearning4j.zoo.model.LeNet = org.deeplearning4j.zoo.model.LeNet@5bcc22b9\n"}]},"apps":[],"jobName":"paragraph_1567057651025_1139882649","id":"20190813-014219_2027485131","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:409","user":"anonymous","dateFinished":"2019-08-29T06:54:01+0000","dateStarted":"2019-08-29T06:53:59+0000"},{"text":"import collection.JavaConverters._\nzooModel.setInputShape(Array(Array(3, 32, 32)))","dateUpdated":"2019-08-29T07:01:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import collection.JavaConverters._\n"}]},"apps":[],"jobName":"paragraph_1567057651025_1139882649","id":"20190828-024936_683229252","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:410","user":"anonymous","dateFinished":"2019-08-29T07:01:25+0000","dateStarted":"2019-08-29T07:01:23+0000"},{"text":"val batchSizePerWorker = 4 \nval tm = new ParameterAveragingTrainingMaster.Builder(batchSizePerWorker)    //Each DataSet object: contains (by default) 32 examples\n  .workerPrefetchNumBatches(0)\n  .saveUpdater(true)\n  .averagingFrequency(5)                            //Do 5 minibatch fit operations per worker, then average and redistribute parameters\n  .batchSizePerWorker(batchSizePerWorker)     //Number of examples that each worker uses per fit operation\n  .build()\n  \nval sparkNet = new SparkDl4jMultiLayer(sc, zooModel.conf(), tm)","dateUpdated":"2019-08-29T07:01:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"batchSizePerWorker: Int = 4\ntm: org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster = ParameterAveragingTrainingMaster(saveUpdater=true, numWorkers=null, rddDataSetNumExamples=4, averagingFrequency=5, aggregationDepth=2, prefetchNumBatches=0, iterationCount=0, trainingHookList=null)\nsparkNet: org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer = org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer@349ed80b\n"}]},"apps":[],"jobName":"paragraph_1567057651025_1139882649","id":"20190819-011525_331240530","dateCreated":"2019-08-29T05:47:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:411","user":"anonymous","dateFinished":"2019-08-29T07:01:26+0000","dateStarted":"2019-08-29T07:01:25+0000"},{"text":"val numEpochs = 2\nfor (i <- 0 to numEpochs) {\n  val trained = sparkNet.fit(trainData)\n}","dateUpdated":"2019-08-29T06:55:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1567057651025_1139882649","id":"20190813-015330_1330142053","dateCreated":"2019-08-29T05:47:31+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:412","user":"anonymous","dateFinished":"2019-08-29T06:55:50+0000","dateStarted":"2019-08-29T06:55:34+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"numEpochs: Int = 2\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 17, tcc1-w-1.us-central1-c.c.hidden-analyzer-249419.internal): java.lang.IllegalArgumentException: Invalid input: expect output columns must be equal to rows 32 x columns 32 x channels 3 but was instead [4, 4]\n\tat org.deeplearning4j.nn.conf.preprocessor.FeedForwardToCnnPreProcessor.preProcess(FeedForwardToCnnPreProcessor.java:94)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:788)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2224)\n\tat org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:174)\n\tat org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)\n\tat org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1780)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1729)\n\tat org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1832)\n\tat org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:179)\n\tat org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:41)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:125)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:42)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:71)\n\tat org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:38)\n\tat org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)\n  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)\n  at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438)\n  at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.processResults(ParameterAveragingTrainingMaster.java:801)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.doIterationPaths(ParameterAveragingTrainingMaster.java:712)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.executeTrainingPathsHelper(ParameterAveragingTrainingMaster.java:432)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster.executeTraining(ParameterAveragingTrainingMaster.java:317)\n  at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:218)\n  at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:205)\n  at $$$$bec6d1991b88c272b3efac29d720f546$$$$anonfun$1.apply$mcVI$sp(<console>:119)\n  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n  ... 74 elided\nCaused by: java.lang.IllegalArgumentException: Invalid input: expect output columns must be equal to rows 32 x columns 32 x channels 3 but was instead [4, 4]\n  at org.deeplearning4j.nn.conf.preprocessor.FeedForwardToCnnPreProcessor.preProcess(FeedForwardToCnnPreProcessor.java:94)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:788)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2224)\n  at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:174)\n  at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)\n  at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1780)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1729)\n  at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1832)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:179)\n  at org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingWorker.processMinibatch(ParameterAveragingTrainingWorker.java:41)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:125)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerFlatMapAdapter.call(ExecuteWorkerFlatMap.java:42)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:71)\n  at org.deeplearning4j.spark.api.worker.ExecuteWorkerPathFlatMapAdapter.call(ExecuteWorkerPathFlatMap.java:38)\n  at org.datavec.spark.transform.BaseFlatMapFunctionAdaptee.call(BaseFlatMapFunctionAdaptee.java:24)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:86)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"}]}},{"text":"val resultado = sparkNet.doEvaluation(testData, 64, new Evaluation(10))(0)","dateUpdated":"2019-08-29T06:51:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:93: error: not found: value sparkNet\n       val resultado = sparkNet.doEvaluation(testData, 64, new Evaluation(10))(0)\n                       ^\n<console>:93: error: not found: value testData\n       val resultado = sparkNet.doEvaluation(testData, 64, new Evaluation(10))(0)\n                                             ^\n"}]},"apps":[],"jobName":"paragraph_1567057651025_1139882649","id":"20190813-021401_2110373941","dateCreated":"2019-08-29T05:47:31+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:413","user":"anonymous","dateFinished":"2019-08-29T06:51:56+0000","dateStarted":"2019-08-29T06:51:56+0000"},{"text":"println(resultado)","dateUpdated":"2019-08-29T06:51:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:94: error: not found: value resultado\n       println(resultado)\n               ^\n"}]},"apps":[],"jobName":"paragraph_1567057651026_1141036895","id":"20190828-032002_1964617380","dateCreated":"2019-08-29T05:47:31+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:414","user":"anonymous","dateFinished":"2019-08-29T06:51:56+0000","dateStarted":"2019-08-29T06:51:56+0000"},{"dateUpdated":"2019-08-29T05:47:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1567057651026_1141036895","id":"20190828-032119_912714771","dateCreated":"2019-08-29T05:47:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:415"}],"name":"LeNet - Iris","id":"2EPE1WYUA","angularObjects":{"2EMEK7VUM:shared_process":[],"2EPE8T2P4:shared_process":[],"2EP8J6V8B:shared_process":[],"2EJTVPNGW:shared_process":[],"2EKZ5MUY8:shared_process":[],"2EJMGXHDC:shared_process":[],"2ENESSFH2:shared_process":[],"2EMEEVCDS:shared_process":[],"2EJS3R1EF:shared_process":[],"2EJPMY3R8:shared_process":[],"2EJXTRMK5:shared_process":[],"2EK3ZXPMR:shared_process":[],"2EJRF8AP4:shared_process":[],"2ENJKD5RW:shared_process":[],"2EJWMB84A:shared_process":[],"2EKDSAUPF:shared_process":[],"2EM1EHM63:shared_process":[],"2EK3NF67M:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}